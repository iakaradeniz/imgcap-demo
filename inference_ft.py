# inference.py

from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import sys, torch

# â€”â€”â€” 1) Sabitler â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# EÄŸer yereldeki blip_ft/ klasÃ¶rÃ¼ndeki model dosyalarÄ±nÄ± kullanacaksan:
FINE_TUNE_PATH = "iakaradeniz/blip-flickr8k-ft"                           

# Base processor hÃ¢lÃ¢ Hugging Faceâ€™den:
BASE_PROC      = "Salesforce/blip-image-captioning-base"  

# â€”â€”â€” 2) Cihaz ayarÄ± â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# â€”â€”â€” 3) Processor & Model â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
processor = BlipProcessor.from_pretrained(BASE_PROC)
model     = BlipForConditionalGeneration.from_pretrained(FINE_TUNE_PATH)
model     = model.to(device).eval()

# â€”â€”â€” 4) Caption fonksiyonu â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
@torch.no_grad()
def caption(img_path: str, beams: int = 5, max_len: int = 30) -> str:
    image  = Image.open(img_path).convert("RGB")
    inputs = processor(images=image, return_tensors="pt").to(device)
    output = model.generate(**inputs, num_beams=beams, max_length=max_len)
    return processor.decode(output[0], skip_special_tokens=True)

# â€”â€”â€” 5) Ana blok â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python inference.py <image_path>")
        sys.exit(1)

    img_path = sys.argv[1]
    print("ğŸ–¼  GÃ¶rsel :", img_path)
    print("ğŸ“ Caption:", caption(img_path))
